$$ \nabla_\boldsymbol{x} J(\boldsymbol{x}) $$

# Characterizing the Structure of English Sentences by Examining How a Language Model Makes Its Predictions

## Table of Contents

1. [Motivation](#motivation)
2. [Background: Feature Interactions](#background-feature-interactions)
3. [Footnotes](#footnotes)

## Motivation

Why does a language model make the predictions it does? Suppose you were asked to predict the next word in this sentence:

> yesterday my buddies and I stayed up all night watching the

What would you choose? Take a moment and think. The GPT-2 language model predicts these as the twenty most likely next words:

> news game new first Super sun movie final World latest last games big show sunset fireworks Olympics NFL sunrise NBA

Most likely, many of these were the same predictions that you thought of. Indeed, modern language models have gotten so good that they are often indistinguishable from human performance. Because language models so closely approximate human performance, understanding why language models make the predictions they do will also tell us about how humans predict subsequent words in a sentence. It can help us characterize the structure of language. In this post, we will first examine the concept of feature interactions, and then we will use them to explain how language models make the predictions that they do.

## Background: Feature Interactions

Every effective machine learning model must learn an accurate, generalizable mapping from its inputs to a prediction. For many problems, a linear combination of the input features is insufficient. A model must instead focus on learning complex interactions between these features. A great way to understand a trained model is thus by examining the feature interactions that it has learned, and which it uses to make predictions.

Tsang et al. [^1] recently developed `Archipelago`, a method to discover the features interactions a model has learned. We will first examine its core ideas, and then apply it to the GPT-2 language model.

Archipelago starts by detecting all pairs of inputs `(a, b)` which "interact." Intuitively, this means that the effect on the model's prediction of seeing `a` and `b` together is different than the effect of only seeing `a` added to the effect of only seeing `b`. More formally, we might say that `a` and `b` exhibit a positive interaction when

    f(a, b) > f(a, _) + f(_, b)
 
 where `_` is a baseline value representing an absence of signal. However, this equation is not quite right. To see why, imagine that we are predicting a person's height based on two features `a` and `b`. Suppose that
 
    f(a, b) = 7
    f(a, _) = 4
    f(_, b) = 4
    
In this case, we inituitively feel that `a` and `b` exhibit a strong positive interaction. Alone, they each predict that the person is very short, while taken together they predict that the person is very tall. However, `f(a, b) `$\ngtr$` 7 >/ b`
    
$$
\ngtr
$$

## Footnotes

[^1]: Tsang, Michael, et al. “How Does This Interaction Affect Me? Interpretable Attribution for Feature Interactions.” *ArXiv:2006.10965 [Cs, Stat]*, June 2020. *arXiv.org*, http://arxiv.org/abs/2006.10965.
