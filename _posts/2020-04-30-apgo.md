# Characterizing the Structure of English Sentences by Examining How a Language Model Makes Its Predictions

## Table of Contents

1. [Motivation](#motivation)
2. [Background: Feature Interactions](#background-feature-interactions)
3. [Footnotes](#footnotes)

## Motivation

Why does a language model make the predictions it does? Suppose you were asked to predict the next word in this sentence:

> yesterday my buddies and I stayed up all night watching the

What would you choose? Take a moment and think. The GPT-2 language model predicts these as the twenty most likely next words:

> news game new first Super sun movie final World latest last games big show sunset fireworks Olympics NFL sunrise NBA

Most likely, many of these were the same predictions that you thought of. Indeed, modern language models have gotten so good that they are often indistinguishable from human performance. Because language models so closely approximate human performance, understanding why language models make the predictions they do will also tell us about how humans predict subsequent words in a sentence. It can help us characterize the structure of language. In this post, we will first examine the concept of feature interactions, and then we will use them to explain how language models make the predictions that they do.

## Background: Feature Interactions

Every effective machine learning model must learn an accurate, generalizable mapping from its inputs to a prediction. For many problems, a linear combination of the input features is insufficient. A model must instead focus on learning complex interactions between these features. A great way to understand a trained model is thus by examining the feature interactions that it has learned, and which it uses to make predictions.

Tsang et al. [^1].

## Footnotes

[^1]: That is great

Tsang, Michael, et al. “How Does This Interaction Affect Me? Interpretable Attribution for Feature Interactions.” *ArXiv:2006.10965 [Cs, Stat]*, June 2020. *arXiv.org*, http://arxiv.org/abs/2006.10965.
