# Characterizing the Structure of English Sentences by Examining How a Language Model Makes Its Predictions

TODOs
- Get footnotes to work
- Get latex to work (for the not-greater symbol)

## Table of Contents

1. [Motivation](#motivation)
2. [Background: Feature Interactions](#background-feature-interactions)
3. [Footnotes](#footnotes)

## Motivation

Why does a language model make the predictions it does? Suppose you were asked to predict the next word in this sentence:

> yesterday my buddies and I stayed up all night watching the

What would you choose? Take a moment and think. The GPT-2 language model predicts these as the twenty most likely next words:

> news game new first Super sun movie final World latest last games big show sunset fireworks Olympics NFL sunrise NBA

Most likely, many of these were the same predictions that you thought of. Indeed, modern language models have gotten so good that they are often indistinguishable from human performance. Because language models so closely approximate human performance, understanding why language models make the predictions they do will also tell us about how humans predict subsequent words in a sentence. It can help us characterize the structure of language. In this post, we will first examine the concept of feature interactions, and then we will use them to explain how language models make the predictions that they do.

## Background: Feature Interactions

Every effective machine learning model must learn an accurate, generalizable mapping from its inputs to a prediction. For many problems, a linear combination of the input features is insufficient. A model must instead focus on learning complex interactions between these features. A great way to understand a trained model is thus by examining the feature interactions that it has learned, and which it uses to make predictions.

Tsang et al. [^1] recently developed `Archipelago`, a method to discover the features interactions a model has learned. We will first examine its core ideas, and then apply it to the GPT-2 language model.

Archipelago starts by detecting all pairs of inputs `(a, b)` which "interact." Intuitively, this means that the effect on the model's prediction of seeing `a` and `b` together is different than the effect of only seeing `a` added to the effect of only seeing `b`. More formally, we might say that `a` and `b` exhibit a positive interaction when

    f(a, b) > f(a, _) + f(_, b)
 
 where `_` is a baseline value representing an absence of signal. However, this equation is not quite right. To see why, imagine that we are predicting a person's height based on two features `a` and `b`. Suppose that
 
    f(a, b) = 8
    f(a, _) = 4
    f(_, b) = 4
    
In this case, we inituitively feel that `a` and `b` exhibit a strong positive interaction. Alone, they each predict that the person is very short, while taken together they predict that the person is very tall. However, `f(a, b) = f(a, _) + f(_, b) = 8` which indicates no interaction. This discrepancy is due to fact that our model's baseline prediction is `f(_, _) = 5.5` (roughly the average human height). This baseline prediction is included once on the left side of the equation, but twice on the right. We can thus fix our equation by subtracting it off like so:

    f(a, b) > f(a, _) + f(_, b) - f(_, _)

We can also re-arrange this equation as follows to get a different perspective on what an interaction represents:

    f(a, b) - f(_, b) > f(a, _) + f(_, _)

Intuitively, this means that observing feature `a` in the presence of feature `b` has a different effect on the model's prediction than the effect of seeing feature `a` in the presence of the baseline. Tsang et al. provide this helpful visualization:

![](/images/apgo_corners.png)

Here, we are trying to predict the sentiment of the sentence `bad, awful`. Observe that seeing `bad` in the context of the baseline (`f(bad, _) - f(_, _)`) has a strong negative effect, whereas seeing `bad` in the context of `awful` (`f(bad, awful) - f(_, awful)`) has a neutral effect, since the sentiment of the sentiment is already negative, and adding `bad` does not change it much. Thus we conclude that

    f(bad, awful) - f(_, awful) > f(bad, _) - f(_, _)

which demonstrates that `bad` and `awful` exhibit a positive interaction. We now have two equivalent ways to conceptualize a positive interaction between a pair of features:

1. `f(a, b) > f(a, _) + f(_, b) - f(_, _)`. The effect of seeing `a` and `b` together is greater than the sum of the effects of seeing them individually,
2. `f(a, b) - f(_, b) > f(a, _) + f(_, _)`. Th effect of seeing `a` in the context of `b` is different than the effect of seeing `a` alone.

## How `Archipelago` Detects Feature Interactions

## Footnotes

[^1]: Tsang, Michael, et al. “How Does This Interaction Affect Me? Interpretable Attribution for Feature Interactions.” *ArXiv:2006.10965 [Cs, Stat]*, June 2020. *arXiv.org*, http://arxiv.org/abs/2006.10965.
